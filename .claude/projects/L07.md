# L07: Embedding Model Comparison

**Thesis**: `research/system_theses/SUMMARY.md` (L07 section)
**Priority**: Medium
**Status**: done

## Overview
Compare embedding models for speed vs quality tradeoffs. Find optimal model for different use cases.

## Available Models (4GB VRAM)
| Model | Dimensions | Size | Use Case |
|-------|------------|------|----------|
| nomic-embed-text | 768 | 274MB | Quality-first |
| all-minilm | 384 | 45MB | Speed-first |

## Tasks
- [x] Benchmark embedding speed
- [x] Test parallel throughput
- [x] Document recommendations

## Benchmark Results

### Sequential (Single Text)
| Model | Avg Time | Dimensions |
|-------|----------|------------|
| all-minilm | ~2.3s | 384 |
| nomic-embed-text | ~2.3s | 768 |

**Finding**: Sequential embedding is similar speed for both models.

### Parallel (10 texts, 10 workers)
| Model | Total Time | Throughput |
|-------|------------|------------|
| all-minilm | 2.18s | 4.6/sec |
| nomic-embed-text | 2.34s | 4.3/sec |

**Finding**: Parallel embedding throughput is nearly identical! GPU batching makes both models efficient.

### Key Insights

1. **L01/L03 6.5x difference was misleading** - That was for sequential calls. With ThreadPoolExecutor (L01 optimization), both models batch on GPU and achieve similar throughput.

2. **Quality difference matters**:
   - nomic-embed-text (768d): Better semantic capture
   - all-minilm (384d): 50% smaller vectors, lower quality

3. **Storage impact**:
   - 768d vector: ~6KB per point
   - 384d vector: ~3KB per point
   - For 10,000 points: 60MB vs 30MB difference

## Recommendations

**Use nomic-embed-text (current default)** for:
- Research storage (lineage-research, lineage-consult)
- Semantic search where quality matters
- Any retrieval where precision is important

**Consider all-minilm** for:
- Very large codebases (storage savings)
- Preliminary/draft indexing
- Speed-critical batch operations where quality is secondary

## Model Selection Option

Scripts already support `--model` or model parameter:
```python
# In existing scripts
def get_ollama_embedding(text, model="nomic-embed-text"):
```

To switch models, pass model parameter. No code changes needed.

## Handoff
Complete. Benchmark shows parallel embedding throughput is similar. Recommend keeping nomic-embed-text as default for quality. Model selection already supported in scripts.

---
**Started**: 2026-01-16
**Completed**: 2026-01-16
**Owner**: @claude
