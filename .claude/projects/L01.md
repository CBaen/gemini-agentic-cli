# L01: Ollama Embedding Optimization

**Thesis**: `research/system_theses/SUMMARY.md` (L01 section)
**Priority**: High
**Status**: done

## Overview
Optimize Ollama embedding speed from 2.4s to <500ms per embedding via GPU acceleration and batch processing.

## Tasks
- [x] Enable GPU for Ollama (set OLLAMA_NUM_GPU=1)
- [x] Verify GPU is being used (model in VRAM confirmed)
- [x] Implement ThreadPoolExecutor for batch embedding
- [x] Test embedding speed improvement
- [x] Document changes

## Results
- **GPU enabled**: `setx OLLAMA_NUM_GPU 1` - model now loads to VRAM
- **Individual embedding**: ~2.15s (GPU doesn't dramatically speed up small models)
- **Parallel embedding**: **15x+ speedup** with optimal ThreadPoolExecutor settings
  - 1 worker (sequential): 33.14s for 16 embeddings
  - 4 workers: 8.32s (4x speedup)
  - 8 workers: 4.21s (8x speedup)
  - 16 workers: 2.22s (15x speedup)
  - 32 workers: 2.36s for 32 embeddings (GPU batching ceiling)

  **Key finding**: The GPU can batch ~32 embeddings in parallel, completing in ~2.4s total.
  This is the hardware limit for this T600 GPU.

## Changes Made
- Modified `~/.claude/scripts/qdrant-chunked-store.py`:
  - Added `concurrent.futures.ThreadPoolExecutor`
  - Embeddings now generated in parallel with `min(chunk_count, 32)` workers
  - **~15x throughput improvement** for batch operations

## Handoff
Complete. Key finding: Individual embedding speed is bottlenecked by Ollama's HTTP overhead, not GPU.
The real win is parallelization for batch operations.

---
**Started**: 2026-01-16
**Completed**: 2026-01-16
**Owner**: @claude
